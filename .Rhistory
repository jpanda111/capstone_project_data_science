library(rpart)
library(rattle)
tree <- train(classe ~ ., data=myTraining, method="rpart")
print(tree$finalModel)
fancyRpartPlot(tree$finalModel)
knitr::opts_chunk$set(echo = TRUE, results = "asis",tidy = TRUE,include = TRUE,cache = TRUE)
library(datasets)
library(lattice)
library(ggplot2)
library(xtable)
library(caret)
library(knitr)
library(rpart)
library(rattle)
library(rpart.plot)
library(randomForest)
Sys.setlocale("LC_ALL","English")
sessionInfo()
fancyRpartPlot(tree$finalModel)
myTesting
dim(myTesting)
predtree <- predict(tree,newdata = myValidation)
confusionMatrix(predtree,myValidation$classe)
predtree <- predict(tree,newdata = myValidation,type = "class")
confusionMatrix(predtree,myValidation$classe)
dim(myValidation)
set.seed(33321)
tree <- train(classe ~ ., data=myTraining, method="rpart")
## print(tree$finalModel)
fancyRpartPlot(tree$finalModel)
predtree <- predict(tree,newdata = myValidation,type = "class")
confusionMatrix(predtree,myValidation$classe)
rf <- train(classe ~., data=myTraining, method="rf",prox=TRUE)
rf <- train(classe ~., data=myTraining, method="rf",prox=TRUE,ntree=500)
tree <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(tree)
cfm <- confusionMatrix(predtree,myValidation$classe)
plot(cfm$table, col=cfm$byClass, main=paste("Decision Tree Confusion Matrix: Accuracy=", round(cfm$overall['Accuracy'],4)))
cfm
head(myValidation)
head(myTraining)
head(myTesting)
head(testing)
testing$classe
testing <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),na.string=c("NA","#DIV/0!"))
head(testing)
plot(rf)
confusionMatrix(predrf,myValidation$classe)
?confusionMatrix
library(caret)
library(lattice)
library(ggplot2)
confusionMatrix(predrf,myValidation$classe)
rf
predrf <- predict(rf, newdata = myValidation)
confusionMatrix(predrf,myValidation$classe)
ï¼Ÿpredict
?predict
predrf<-predict(rf,myValidation)
predrf<-predict(rf,myValidation,type="class")
rf$finalModel
rf$confusion
rf$classes
rf <- randomForest(classe ~., data=myTraining)
library(randomForest)
rf <- randomForest(classe ~., data=myTraining)
head(getTree(rf$finalModel,k=2))
predrf <- predict(rf, newdata = myValidation)
confusionMatrix(predrf,myValidation$classe)
plot(rf)
gbm <- train(classe ~., data=myTraining, method="gbm",verbose=FALSE)
library(gbm)
library(survival)
library(splines)
library(parallel)
gbm <- train(classe ~., data=myTraining, method="gbm",verbose=FALSE)
?trainControl
?gbm
fitControl <- trainControl(method ="repeatedcv", number=5, repeats=1)
fitControl
gbm <- train(classe~.,data=myTraining,method="gbm",trControl=fitControl,verbose=FALSE)
predict(rf, myTesting)
predrf <- predict(rf, newdata = myValidation)
confusionMatrix(predrf,myValidation$classe)
plot(rf)
predict(rf, myTesting)
summary(myTesting)
summary(myValidation)
summary(myTraining)
myTesting$user_name
myTraining$user_name
gbm <- train(classe~.,data=myTraining,method="gbm",trControl=fitControl,verbose=FALSE)
common<-intersect(names(myTraining),names(myTesting)) for (p in common) {if (class(myTraining[p])=="factor") {level(myTesting[p])<-level(myTraining[p])}}
common<-intersect(names(myTraining),names(myTesting))
common
for (p in common) {if (class(myTraining[p])=="factor") {level(myTesting[p])<-level(myTraining[p])}}
predict(rf, myTesting)
myTesting
head(myTesting,1)
myTesting[1,]
xtest<-rbind(myTraining[1,],myTesting)
xtest<-rbind(myTraining[1,],xtest)
myTesting<-rbind(myTraining[1,-58],myTesting)
myTesting<-myTesting[-1,]
predict(rf, myTesting)
lda <- train(classe ~., data=myTraining, method="lda")
print(lda)
predlda<-predict(lda,newdata=myValidation)
predlda
confusionMatrix(predlda,myValidation$classe)
nb<-train(classe~.,data=myTraining,method="nb")
library(klaR)
library(MASS)
nb<-train(classe~.,data=myTraining,method="nb")
confusionMatrix(predrf,myValidation$classe)
install.packages("doMC")
doMC::registerDoMC(cores=4)
?doMC
library(doMC)
install.packages("doSMP")
install.packages("doMC",repos="http://R-Forge.R-project.org")
library(doMC)
library(foreach)
library(iterators)
library(doMC)
registerDoMC(cores=4)
lda <- train(classe ~., data=myTraining, method="lda", preProcess=c("center","scale"))
print(lda)
predlda <- predict(lda, newdata = myValidation)
confusionMatrix(predlda,myValidation$classe)
## Naive Bayes
nb <- train(classe ~., data=myTraining, method="nb", preProcess=c("center","scale"))
print(nb)
prednb <- predict(nb, newdata = myValidation)
confusionMatrix(prednb, myValidation$classe)
.rs.enableRStudioConnectUI(TRUE)
corrPlot <- cor(myTraining[,-length(names(myTraining))])
myTraining
corrPlot <- cor(myTraining[,-length(names(myTraining))])
length(names(myTraining))
corrPlot <- cor(myTraining[,-58])
?cor
class(myTraining[,-58])
summary(myTraining)
trainRemove <- grepl("^X|timestamp|window", names(myTraining))
trainRemove
myTraining <- myTraining[,!trainRemove]
corrPlot <- cor(myTraining[,-length(names(myTraining))])
myTraining <- myTraining[, sapply(myTraining, is.numeric)]
corrPlot <- cor(myTraining[,-length(names(myTraining))])
corrplot(corrPlot, method="color")
library(corrplot)
corrplot(corrPlot, method="color")
fancyRpartPlot(tree, tweak=1)
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rattle)
library(rpart.plot)
library(randomForest)
library(gbm)
library(survival)
library(splines)
library(parallel)
library(klaR)
library(MASS)
fancyRpartPlot(tree, tweak=1)
fancyRpartPlot(tree)
fancyRpartPlot(tree, tweak=1)
prp(tree)
correlationMatrix<-cor(training)
correlationMatrix<-cor(myTraining)
highCorIdx <- findCorrelation(correlationMatrix, cutoff=0.75)
highCorIdx
summary(myTraining)
myTraining <- newTraining[,-Matrix]
summary(myTraining)
myTraining <- myTraining[,-1]
summary(myTraining)
trainRemove <- grepl("^X|timestamp|window", names(myTraining))
myTraining <- myTraining[,!trainRemove]
summary(myTraining)
myTraining[,-58]
head(myTraining[,-58],2)
head(myTraining,2)
head(myTraining[,-57],2)
head(myTraining[,-1],2)
head(myTraining[,-56],2)
head(myTraining[,-length(names(myTraining))],2)
length(myTraining)
myTraining <- myTraining[, sapply(myTraining[,-length(names(myTraining))], is.numeric)]
summary(myTraining)
myTraining <- newTraining[,-Matrix]
myTraining <- myTraining[,-1]
trainRemove <- grepl("^X|timestamp|window", names(myTraining))
myTraining <- myTraining[,!trainRemove]
myTraining[, -length(names(myTraining))] <- myTraining[, sapply(myTraining[,-length(names(myTraining))], is.numeric)]
summary(myTraining)
c1 <- colnames(myTraining)
c2 <- colnames(myTraining[,-length(names(myTraining))])
c2
c1
correlationMatrix <- cor(myTraining)
correlationMatrix <- cor(myTraining[,-length(names(myTraining))])
highCorIndx<-findCorrelation(correlationMatrix,cutoff=0.75)
highCorIndx
myTraining <- myTraining[,-highCorIdx]
myValidation <- myValidation[,-highCorIdx]
myTesting <- myTesting[,-highCorIdx]
summary(myTesting)
summary(myTraining)
summary(myValidation)
dim(myTraining);dim(myValidation);dim(myTesting)
myValidation <- myValidation[,!trainRemove]
myTesting <- myTesting[,!trainRemove]
myValidation <- newValidation[,!trainRemove]
myTesting <- testing[,!trainRemove]
myValidation <- myValidation[,-highCorIdx]
myTesting <- myTesting[,-highCorIdx]
dim(myTraining);dim(myValidation);dim(myTesting)
myValidation <- newValidation[c1]
myTesting <- testing[c2]
c1 <- colnames(myTraining)
c2 <- colnames(myTraining[,-length(names(myTraining))])
myValidation <- newValidation[c1]
myTesting <- testing[c2]
dim(myTraining);dim(myValidation);dim(myTesting)
summary(myTraining)
summary(myValidation)
summary(testing)
head(newTraining)
tree <- train(classe ~ ., data=myTraining, method="rpart", trControl=fitControl)
fancyRpartPlot(tree$finalModel)
predtree <- predict(tree,newdata = myValidation)
confusionMatrix(predtree,myValidation$classe)$overall
set.seed(33321)
tree <- train(classe ~ ., data=myTraining, method="rpart", trControl=fitControl)
fancyRpartPlot(tree$finalModel)
predtree <- predict(tree,newdata = myValidation)
confusionMatrix(predtree,myValidation$classe)$overall
varImp(tree,scale=FALSE)
var_importance <- varImp(tree,scale=FALSE)
plot(var_importance)
rf <- randomForest(classe ~., data=myTraining,trControl=fitControl)
predrf <- predict(rf, newdata = myValidation)
confusionMatrix(predrf,myValidation$classe)
knitr::opts_chunk$set(echo = TRUE, results = "asis",tidy = TRUE,include = TRUE,cache = TRUE)
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rattle)
library(rpart.plot)
library(randomForest)
library(gbm)
library(survival)
library(splines)
library(parallel)
library(klaR)
library(MASS)
install.packages("doMC",repos="http://R-Forge.R-project.org")
library(foreach)
library(iterators)
library(doMC)
registerDoMC(cores=4)
Sys.setlocale("LC_ALL","English")
sessionInfo()
```
###2. load data and perform some basic data summary and processing
We removed all the possible unnecessary variables (i.e. NA, zero-variance or highly correlated) and reduce the input predictors from 159 to 38. We did the exactly same transform for validation and test data.
```{r dataanalysis, include=TRUE,warning=FALSE,message=FALSE}
## 1) for reproducible purpose
set.seed(33321)
## 2) load the data
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),na.string=c("NA","#DIV/0!"))
testing <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),na.string=c("NA","#DIV/0!"))
## 3) split the training data into two portions, trainning and validation
inTrain <- createDataPartition(training$classe,p=0.75,list=FALSE)
newTraining <- training[inTrain,]
newValidation <- training[-inTrain,]
## 4) remove zero covariates or if NA's too many for training set.
nzv <- nearZeroVar(newTraining,saveMetrics = TRUE)
newTraining <- newTraining[,nzv$nzv=="FALSE"]
n <- ncol(newTraining)
m <- nrow(newTraining)
list <- list()
for (i in 1:n) {
if (sum(is.na(newTraining[,i]))/m > 0.8 )
list[[i]] <- i
}
Matrix = do.call(cbind,list)
myTraining <- newTraining[,-Matrix]
myTraining <- myTraining[,-1]
## 5) remove the measurement time stamp or windows
trainRemove <- grepl("^X|timestamp|window", names(myTraining))
myTraining <- myTraining[,!trainRemove]
myTraining[, -length(names(myTraining))] <- myTraining[, sapply(myTraining[,-length(names(myTraining))], is.numeric)]
## 6) eliminate highly correlated variables
correlationMatrix <- cor(myTraining[,-length(names(myTraining))])
highCorIdx<-findCorrelation(correlationMatrix,cutoff = 0.75)
myTraining <- myTraining[,-highCorIdx]
## 7) do the same transformation for validation set and test set
c1 <- colnames(myTraining)
c2 <- colnames(myTraining[,-length(names(myTraining))])
myValidation <- newValidation[c1]
myTesting <- testing[c2]
dim(myTraining);dim(myValidation);dim(myTesting)
```
fitControl <- trainControl(method ="repeatedcv", number=5, repeats=1)
fitControl <- trainControl(method ="repeatedcv", number=10, repeats=2)
tree <- train(classe ~ ., data=myTraining, method="rpart", trControl=fitControl)
fancyRpartPlot(tree$finalModel)
predtree <- predict(tree,newdata = myValidation)
confusionMatrix(predtree,myValidation$classe)$overall
varImp(tree,scale=FALSE)
rf <- randomForest(classe ~., data=myTraining,trControl=fitControl)
predrf <- predict(rf, newdata = myValidation)
confusionMatrix(predrf,myValidation$classe)$overall
plot(rf)
varImp(rf,scale=FALSE)
gbm <- train(classe ~., data=myTraining, method="gbm",trControl=fitControl,verbose=FALSE)
varImpPlot(tree$finalModel)
varImpPlot(rf$finalModel)
class(rf)
varImpPlot(rf)
rf <- randomForest(classe ~., data=myTraining)
predrf <- predict(rf, newdata = myValidation)
confusionMatrix(predrf,myValidation$classe)$overall
knitr::opts_chunk$set(echo = TRUE, results = "asis",tidy = TRUE,include = TRUE,cache = TRUE)
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rattle)
library(rpart.plot)
library(randomForest)
library(gbm)
library(survival)
library(splines)
library(parallel)
library(klaR)
library(MASS)
install.packages("doMC",repos="http://R-Forge.R-project.org")
library(foreach)
library(iterators)
library(doMC)
registerDoMC(cores=4)
Sys.setlocale("LC_ALL","English")
sessionInfo()
```
###2. load data and perform some basic data summary and processing
We removed all the possible unnecessary variables (i.e. NA, zero-variance or highly correlated) and reduce the input predictors from 159 to 38. We did the exactly same transform for validation and test data.
```{r dataanalysis, include=TRUE,warning=FALSE,message=FALSE}
## 1) for reproducible purpose
set.seed(33321)
## 2) load the data
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),na.string=c("NA","#DIV/0!"))
set.seed(33321)
## 2) load the data
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),na.string=c("NA","#DIV/0!"))
testing <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),na.string=c("NA","#DIV/0!"))
## 3) split the training data into two portions, trainning and validation
inTrain <- createDataPartition(training$classe,p=0.75,list=FALSE)
newTraining <- training[inTrain,]
newValidation <- training[-inTrain,]
## 4) remove zero covariates or if NA's too many for training set.
nzv <- nearZeroVar(newTraining,saveMetrics = TRUE)
newTraining <- newTraining[,nzv$nzv=="FALSE"]
n <- ncol(newTraining)
m <- nrow(newTraining)
list <- list()
for (i in 1:n) {
if (sum(is.na(newTraining[,i]))/m > 0.8 )
list[[i]] <- i
}
Matrix = do.call(cbind,list)
myTraining <- newTraining[,-Matrix]
myTraining <- myTraining[,-1]
## 5) remove the measurement time stamp or windows
trainRemove <- grepl("^X|timestamp", names(myTraining))
myTraining <- myTraining[,!trainRemove]
myTraining[, -length(names(myTraining))] <- myTraining[, sapply(myTraining[,-length(names(myTraining))], is.numeric)]
## 7) do the same transformation for validation set and test set
c1 <- colnames(myTraining)
c2 <- colnames(myTraining[,-length(names(myTraining))])
myValidation <- newValidation[c1]
myTesting <- testing[c2]
dim(myTraining);dim(myValidation);dim(myTesting)
fitControl <- trainControl(method ="repeatedcv", number=5, repeats=2)
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rattle)
library(rpart.plot)
library(randomForest)
library(gbm)
library(survival)
library(splines)
library(parallel)
library(klaR)
library(MASS)
install.packages("doMC",repos="http://R-Forge.R-project.org")
library(foreach)
library(iterators)
library(doMC)
registerDoMC(cores=4)
Sys.setlocale("LC_ALL","English")
sessionInfo()
fitControl <- trainControl(method ="repeatedcv", number=5, repeats=2)
rf <- randomForest(classe ~., data=myTraining, trControl=fitControl)
predrf <- predict(rf, newdata = myValidation)
confusionMatrix(predrf,myValidation$classe)$overall
myTraining <- newTraining[,-Matrix]
myTraining <- myTraining[,-1]
## 5) remove the measurement time stamp or windows
trainRemove <- grepl("^X|timestamp", names(myTraining))
myTraining <- myTraining[,!trainRemove]
## 6) do the same transformation for validation set and test set
c1 <- colnames(myTraining)
c2 <- colnames(myTraining[,-length(names(myTraining))])
myValidation <- newValidation[c1]
myTesting <- testing[c2]
dim(myTraining);dim(myValidation);dim(myTesting)
rf <- randomForest(classe ~., data=myTraining, trControl=fitControl)
predrf <- predict(rf, newdata = myValidation)
confusionMatrix(predrf,myValidation$classe)$overall
myTraining <- newTraining[,-Matrix]
myTraining <- myTraining[,-1]
## 5) remove the measurement time stamp or windows
trainRemove <- grepl("^X|timestamp", names(myTraining))
myTraining <- myTraining[,!trainRemove]
## 6) eliminate highly correlated variables
correlationMatrix <- cor(myTraining[,-length(names(myTraining))])
highCorIdx<-findCorrelation(correlationMatrix,cutoff = 0.75)
myTraining <- myTraining[,-highCorIdx]
## 7) do the same transformation for validation set and test set
c1 <- colnames(myTraining)
c2 <- colnames(myTraining[,-length(names(myTraining))])
myValidation <- newValidation[c1]
myTesting <- testing[c2]
dim(myTraining);dim(myValidation);dim(myTesting)
rf <- randomForest(classe ~., data=myTraining, trControl=fitControl)
predrf <- predict(rf, newdata = myValidation)
confusionMatrix(predrf,myValidation$classe)$overall
head(myTraining)
class(myTraining)
class(myTraining[,])
class(myTraining[,1:54])
class(myTraining[,1:53])
dim(myTraining)
class(myTraining[,1:34])
class(myTraining[,1])
class(myTraining[,2])
summary(myTraining)
cor(as.numeric(myTraining[,-length(names(myTraining))]))
sapply(myTraining[,-34],is.numeric)
numericind<-sapply(myTraining[,-34],is.numeric)
myTraining[,numericind]
summary(myTraining[,numericind])
numericind
myTraining[numericind]
myTraining[numericind]
summary(myTraining[numericind])
numericind
class(numericind)
dim(myTraining)
dim(myTraining[numericind])
dim(myTraining[,numericind])
myTraining <- newTraining[,-Matrix]
myTraining <- myTraining[,-1]
dim(myTraining)
trainRemove <- grepl("^X|timestamp|window", names(myTraining))
myTraining <- myTraining[,!trainRemove]
dim(myTraining)
sapply(myTraining,is.numeric)
numeric_ind<-sapply(myTraining,is.numeric)
numeric_ind
dim(myTraining[numeric_ind])
dim(myTraining)
correlationMatrix <- cor(myTraining[numeric_ind])
highCorIdx<-findCorrelation(correlationMatrix,cutoff = 0.75)
highCorIdx
myTraining <- myTraining[,-highCorIdx]
dim(myTraining)
c1 <- colnames(myTraining)
c2 <- colnames(myTraining[,-length(names(myTraining))])
myValidation <- newValidation[c1]
myTesting <- testing[c2]
dim(myTraining);dim(myValidation);dim(myTesting)
fitControl <- trainControl(method ="repeatedcv", number=5, repeats=1)
rf <- randomForest(classe ~., data=myTraining)
predrf <- predict(rf, newdata = myValidation)
confusionMatrix(predrf,myValidation$classe)$overall
head(myTesting)
predict(rf, myTesting)
predict(rf, myTesting)
dim(myTraining);dim(myValidation);dim(myTesting)
summary(myTesting)
myTraining
shiny::runApp('~/JH_track/Developing_data_products/myApp')
runApp('~/JH_track/Developing_data_products/myApp')
mtcars
runApp('~/JH_track/Developing_data_products/myApp')
setwd("~/GitHub/capstone_project_data_science")
shiny::runApp('~/JH_track/Capstone Project/shiny-app')
install.packages("RSQLite")
library(RSQLite)
runApp('~/JH_track/Capstone Project/shiny-app')
runApp('~/JH_track/Capstone Project/shiny-app')
runApp('~/JH_track/Capstone Project/shiny-app')
